{
  
    
        "post0": {
            "title": "Finetuning DziriBERT for Dialect Detection",
            "content": "DziriBERT is a BERT language model trained and tested on 1.1 Million Algerian tweets, it was introduced in this paper and is available on the Hugging Face Model Hub in this link, which means it is fairly easy to use and finetune the model. In this blog post i&#39;ll show how to finetune DziriBERT for dialect detection. The dataset i will be using is of one the MSDA open datasets, it contains tweets labelled by country-level dialect. All of the five dialects in this dataset have their roots in Arabic, which means that there are lots of similarities between them especially the ones that are geographically close (e.g. Moroccan and Algerian dialects). . We start by installing the transformers library and importing what we need for the implementation. . !pip install -q transformers . |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.9 MB 5.1 MB/s |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 895 kB 32.6 MB/s |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 596 kB 56.7 MB/s |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.3 MB 46.7 MB/s |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 56 kB 4.0 MB/s . import pandas as pd from sklearn.model_selection import train_test_split from pathlib import Path import torch from torch.utils.data import Dataset, DataLoader import transformers from transformers import AutoModel, BertTokenizerFast, AutoModelForSequenceClassification, Trainer, TrainingArguments . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . path = Path(&#39;/content/drive/MyDrive/ml/projects/msda_dataset/dialect&#39;) . MSDA Dialect Detection Dataset . The dataset is available as a csv file and can be found in this link. I want to mention that you can apply the finetuning process in this post on the other MSDA text classification datasets, you just might need to take into consideration that they are imbalanced. . df = pd.read_csv(path/&#39;dialect.csv&#39;) df.tail() . Twits dialect . 53398 Ù‡Ø§ÙŠ Ø¨Ø§Ù‡ÙŠ ÙˆÙ„ÙŠØª ØªÙÙ‡Ù… ÙÙŠ Ø§Ù„Ù„Ø¨Ø³Ø© Ù…ØªØ§Ø¹ Ø§Ù„Ø±Ø¬Ø§Ù„ ğŸ¤”ğŸ¤” | Tunisian | . 53399 Ø±ÙŠÙŠ ÙŠØµØ¨Ø± Ø§Ù…Ù‡Ø§ | Tunisian | . 53400 Ø§Ù„Ù„Ù‡Ù… Ø§Ù…ÙŠÙ† ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ø±Ø¨ÙŠ ÙŠØµØ¨Ø± Ø£Ù‡Ù„Ù‡Ù… | Tunisian | . 53401 Ø§Ù„Ù„Ù‡Ù… Ø§Ù…ÙŠÙ† ÙŠØ§Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠÙ† Ù…Ø®ÙŠØ¨Ù‡Ø§ Ø¹ÙˆØ§Ø´Ø± Ù„Ø§ Ø­ÙˆÙ„ Ùˆ... | Tunisian | . 53402 Ù„Ø§ Ø­ÙˆÙ„ ÙˆÙ„Ø§ Ù‚ÙˆØ© Ø§Ù„Ø§ Ø¨Ø§Ù„Ù„Ù‡ Ø¬Ø±ÙŠÙ…Ø© Ø¨Ø´Ø¹Ø© ÙÙŠ wangen ... | Tunisian | . This is the number of training examples for each class in our dataset. . df.dialect.value_counts().plot.bar(x=df.dialect.unique(), title=&#39;Tweet Distribution&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff349068d90&gt; . We encode the labels by mapping each dialect to an index. . dialects = df[&#39;dialect&#39;].unique() lbl2idx = {d: i for i, d in enumerate(dialects)} df[&#39;dialect&#39;] = df[&#39;dialect&#39;].map(lbl2idx) . df[&#39;dialect&#39;].unique() . array([0, 1, 2, 3, 4]) . We split the dataset randomly, 70% will be used for training, and 30% for validation and testing of the model. . train_texts, temp_texts, train_labels, temp_labels = train_test_split(df[&#39;Twits&#39;], df[&#39;dialect&#39;], random_state=42, test_size=0.3) val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, random_state=42, test_size=0.5) len(train_texts), len(val_texts), len(test_texts) . (37382, 8010, 8011) . DziriBERT tokenizer . We need to specify the name of the model on the Hugging Face Hub, we download first the pretrained DziriBERT tokenizer, which will take care of tokenizing and numeracalizing our sequences as DziriBERT model expects them. . BERT_MODEL_NAME = &#39;alger-ia/dziribert&#39; tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME) . Tokenization is the process of splitting text into tokens, which can be words, characters or subwords. BERT uses an algorithm called WordPiece for tokenization, it&#39;s a subword tokenization algorithm that was pretrained to know which groups of characters to keep together, the most frequent word pieces are the one kept in the vocabulary of the model. We can see bellow that some words are split into pieces while some others are kept as they are. Some tokens are preceded by &quot;##&quot;, this means that that is a completion of a word and should be attached to the previous token when decoding. . tokens = tokenizer.tokenize(train_texts[0]) print(tokens) . [&#39;Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø©&#39;, &#39;Ù†Ø¬ÙˆÙ‰&#39;, &#39;Ù‚Ø§Ù…Ø©&#39;, &#39;ÙÙŠ&#39;, &#39;Ø§Ù„Ù‚ØµÙ&#39;, &#39;Ø§Ù„ØµØ§Ø±&#39;, &#39;##ÙˆØ®&#39;, &#39;##ÙŠ&#39;, &#39;Ù„Ùƒ&#39;, &#39;Ø§Ù†&#39;, &#39;ØªØ±Ù‰&#39;, &#39;Ù…Ù†Ø´ÙˆØ±Ø§Øª&#39;, &#39;##Ù‡Ø§&#39;, &#39;ÙˆØªØ¯Ø®Ù„&#39;, &#39;##Ø§ØªÙ‡Ø§&#39;] . This is the original sequence. . train_texts[0] . &#39;Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© Ù†Ø¬ÙˆÙ‰ Ù‚Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù‚ØµÙ Ø§Ù„ØµØ§Ø±ÙˆØ®ÙŠ Ù„Ùƒ Ø§Ù† ØªØ±Ù‰ Ù…Ù†Ø´ÙˆØ±Ø§ØªÙ‡Ø§ ÙˆØªØ¯Ø®Ù„Ø§ØªÙ‡Ø§&#39; . Before feeding our inputs to the model, we need to convert them to numbers. What we see bellow is the index of each token in the vocabulary of the pretrained DziriBERT WordPiece tokenizer. When finetuning the model, these indices will be used to retrieve the embedding vector of each token that was learned during pretraining. . ids = tokenizer.convert_tokens_to_ids(tokens) print(ids) . [45796, 40053, 48544, 1821, 20759, 41048, 4769, 1016, 2374, 1828, 5601, 19452, 1814, 30404, 3985] . A vocabulary is a dictionary that maps the tokens learned by WordPiece to indices, tokens that are not in the vocabulary are usually replaced with a special token (e.g. [UNK]). . tokenizer.vocab[tokens[0]] . 45796 . encode_plus is a chain of multiple steps to prepare the inputs of our model, this includes the ones we discussed before (tokenize and encode_tokens_to_ids), along with others like padding. We can see it has two outputs, input_ids which is similar to the output of encode_tokens_to_ids, and an another output which is attention_mask, this is used to indicate to the model the position of the padding tokens, this is done so that the attention layers can ignore them when computing token representations. 1 indicates an original token and 0 is for the padding token, which we can&#39;t see in the output because padding is not needed, it is usually useful when batching sequences together and we will see that bellow, we will also see where that 2 and 3 in the input_ids came from. . tokenizer.encode_plus(train_texts[0], return_token_type_ids=False) . {&#39;input_ids&#39;: [2, 45796, 40053, 48544, 1821, 20759, 41048, 4769, 1016, 2374, 1828, 5601, 19452, 1814, 30404, 3985, 3], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]} . We usually want our model to take a batch of sequences, to do that, sequences must have the same length, one way we can do this is to grab the longest sequence in a batch and pad all the other sequences to have the same length. . batch_encode_plus encodes a batch of sequences, we can see that the second sentence is padded with zeros to the length of the longest sequence in the batch, which is the first sentence, and its attention_mask also contains zeros indicating which tokens are padding. . tokenizer.batch_encode_plus(train_texts[:2].to_list(), padding=True, return_token_type_ids=False) . {&#39;input_ids&#39;: [[2, 7654, 5405, 4434, 5699, 20924, 2039, 20735, 2738, 8576, 7893, 1846, 22439, 5793, 24691, 36283, 43820, 6066, 2464, 3], [2, 4473, 2000, 5023, 1843, 21960, 1823, 1821, 5301, 4481, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0]], &#39;attention_mask&#39;: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]} . Decoding is reversing the operations that were applied to the text, but we see that there are two new tokens ([CLS] and [SEP]) in our sequence, these are called special tokens and their indices in the vocabulary are 2 and 3, they are added by the tokenizer and to explain why, i will talk a bit about the training objectives of BERT. . BERT language models are usually trained with two training objectives, Masked Language Modelling (MLM) and Next Sentence Prediction (NSP). . Masked Language Modelling (MLM) is used in most if not all tranformer based models, at first, we mask a percentage of the input tokens, 15% in the original BERT but 25% in the case of DziriBERT as they state in their paper, the reason as they mention is that DziriBERT is trained on tweets and they have a short length. The model is asked to predict the masked tokens based on their context (non-masked tokens), this way the model will learn language representations that can be useful when finetuning on downstream tasks. . Next Sentence Prediction is a classification task where BERT takes a pair of sentences, and tries to predict whether the second sentence is the next sentence or not. This was shown to increase the performance of BERT especially in tasks that involve pairs of sentences (e.g. Question Answering (QA), Natural Language Inference (NLI), ...) as was declared in the BERT paper. The pairs of sentences are separated by [SEP] token, and you can see bellow how the input of BERT should look like when doing NSP. The [CLS] token is used at the beginning of each input, as BERT outputs a vector representation for each token in the sequence, it will use only the representation of the [CLS] token to do Next Sentence Prediction. This way, the [CLS] token is thought to encode information about the whole input sequence, and that is why it is usually used in text classification tasks like ours. . . Note: DziriBERT is trained without NSP objective, and they cite in the paper that it was shown that it does not improve the results of downstream tasks. . out = tokenizer.encode_plus(train_texts[0], return_token_type_ids=False) tokenizer.decode(out[&#39;input_ids&#39;]) . &#39;[CLS] Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© Ù†Ø¬ÙˆÙ‰ Ù‚Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù‚ØµÙ Ø§Ù„ØµØ§Ø±ÙˆØ®ÙŠ Ù„Ùƒ Ø§Ù† ØªØ±Ù‰ Ù…Ù†Ø´ÙˆØ±Ø§ØªÙ‡Ø§ ÙˆØªØ¯Ø®Ù„Ø§ØªÙ‡Ø§ [SEP]&#39; . This is what normally the input of BERT would look like during pretraining with an NSP objective. . tok_out = tokenizer(train_texts[0], train_texts[2]) tokenizer.decode(tok_out[&#39;input_ids&#39;]) . &#39;[CLS] Ø§Ù„Ø¯ÙƒØªÙˆØ±Ø© Ù†Ø¬ÙˆÙ‰ Ù‚Ø§Ù…Ø© ÙÙŠ Ø§Ù„Ù‚ØµÙ Ø§Ù„ØµØ§Ø±ÙˆØ®ÙŠ Ù„Ùƒ Ø§Ù† ØªØ±Ù‰ Ù…Ù†Ø´ÙˆØ±Ø§ØªÙ‡Ø§ ÙˆØªØ¯Ø®Ù„Ø§ØªÙ‡Ø§ [SEP] ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ğŸ˜‚ Ø´ÙƒÙˆÙ† ÙŠØ¹Ø±ÙÙ‡Ø§ğŸ¤” [SEP]&#39; . Finetuning the model . We take a look at the length of sequences in our training set. . seq_len = [len(tokenizer.encode(i)) for i in train_texts] pd.Series(seq_len).hist(bins = 30) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7ff34811a590&gt; . The maximum sequence length that BERT can take is 512, but that would be too much for our case, since most sequences contain less than 40 tokens. We specify that for our tokenizer in max_length and set truncation to true, what this means is that any sequence longer than 40 will be truncated to max_seq_len. With that, we will lose some informations but it will reduce the training time, and that matters a lot when training on free colab ğŸ˜„. . max_seq_len = 40 train_encodings = tokenizer(train_texts.to_list(), truncation=True, padding=True, max_length=max_seq_len) val_encodings = tokenizer(val_texts.to_list(), truncation=True, padding=True, max_length=max_seq_len) test_encodings = tokenizer(test_texts.to_list(), truncation=True, padding=True, max_length=max_seq_len) . We create a Pytorch dataset class, which has two essential methods, __len__ that should return the number of samples in the dataset, and __get_item__ which will return the item (encoding and label) at index idx. . class TweetDataset(Dataset): def __init__(self, encodings, labels): self.encodings = encodings self.labels = labels.to_list() def __getitem__(self, idx): item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()} item[&#39;labels&#39;] = torch.tensor(self.labels[idx]) return item def __len__(self): return len(self.labels) train_dataset = TweetDataset(train_encodings, train_labels) val_dataset = TweetDataset(val_encodings, val_labels) test_dataset = TweetDataset(test_encodings, test_labels) . To evaluate our model while training, we need to pass a function that takes the model&#39;s predictions, and compute the metrics that we care about. In our case we will compute the accuracy, f1 score, precision and recall because our dataset is imbalanced, and also to compare with the baselines in MSDA paper. . from sklearn.metrics import precision_recall_fscore_support, accuracy_score def compute_metrics(pred): labels = pred.label_ids preds = pred.predictions.argmax(-1) precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=&#39;micro&#39;) acc = accuracy_score(labels, preds) return { &#39;accuracy&#39;: acc, &#39;f1&#39;: f1, &#39;precision&#39;: precision, &#39;recall&#39;: recall } . Now is time to load the pretrained DziriBERT model from Hugging Face Hub. What AutoModelForSequenceClassification does is to remove the pretraining head of the model, and replace it with a classification head that will be initialized randomly. This classification head is just a linear layer that will take something called pooler_output as input and output 5 numbers (as specified in num_labels) for each sequence, these are called logits and they are passed through a softmax function to get probabilities when computing the loss or to when computing metrics. . pooler_output is one of the outputs of a BERT model, it is the representation of the [CLS] token after it was passed through a linear layer followed by a tanh activation function. . model = AutoModelForSequenceClassification.from_pretrained(BERT_MODEL_NAME, num_labels=5) . Some weights of the model checkpoint at alger-ia/dziribert were not used when initializing BertForSequenceClassification: [&#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.bias&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.bias&#39;, &#39;cls.predictions.decoder.weight&#39;] - This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of BertForSequenceClassification were not initialized from the model checkpoint at alger-ia/dziribert and are newly initialized: [&#39;bert.pooler.dense.weight&#39;, &#39;classifier.bias&#39;, &#39;classifier.weight&#39;, &#39;bert.pooler.dense.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. . Taking a look at logits. . out = tokenizer.encode_plus(train_texts[0], return_token_type_ids=False, return_tensors=&#39;pt&#39;) model.eval() with torch.no_grad(): print(model(**out)[&#39;logits&#39;]) . tensor([[ 0.0181, -0.0198, -0.2928, 0.1794, 0.1907]]) . The Trainer is what takes care of training our models, it takes arguments through the TrainingArguments class, and takes our model, datasets and the method to compute metrics and output them on a nice table during training. calling train will start finetuning our classification model, and what we need now is to find something to do instead of looking at the progress bar ğŸ˜. . training_args = TrainingArguments( output_dir=&#39;./results&#39;, # output directory num_train_epochs=5, # total number of training epochs per_device_train_batch_size=32, # batch size per device during training per_device_eval_batch_size=64, # batch size for evaluation warmup_steps=500, # number of warmup steps for learning rate scheduler weight_decay=0.01, # strength of weight decay evaluation_strategy=&#39;epoch&#39;, # evaluate at the end of each epoch logging_strategy=&#39;epoch&#39; # log at the end of each epoch ) trainer = Trainer( model=model, # the instantiated Transformers model args=training_args, # training arguments train_dataset=train_dataset, # training dataset eval_dataset=val_dataset, # evaluation dataset compute_metrics=compute_metrics # method we defined before to compute our metrics ) trainer.train() . ***** Running training ***** Num examples = 37382 Num Epochs = 5 Instantaneous batch size per device = 32 Total train batch size (w. parallel, distributed &amp; accumulation) = 32 Gradient Accumulation steps = 1 Total optimization steps = 5845 . . [5845/5845 54:47, Epoch 5/5] Epoch Training Loss Validation Loss Accuracy F1 Precision Recall . 1 | 0.650000 | 0.494892 | 0.826592 | 0.826592 | 0.826592 | 0.826592 | . 2 | 0.300600 | 0.468697 | 0.841199 | 0.841199 | 0.841199 | 0.841199 | . 3 | 0.142800 | 0.587720 | 0.846941 | 0.846941 | 0.846941 | 0.846941 | . 4 | 0.073500 | 0.751222 | 0.853308 | 0.853308 | 0.853308 | 0.853308 | . 5 | 0.041000 | 0.836090 | 0.850312 | 0.850312 | 0.850312 | 0.850312 | . &lt;/div&gt; &lt;/div&gt; Saving model checkpoint to ./results/checkpoint-500 Configuration saved in ./results/checkpoint-500/config.json Model weights saved in ./results/checkpoint-500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-1000 Configuration saved in ./results/checkpoint-1000/config.json Model weights saved in ./results/checkpoint-1000/pytorch_model.bin ***** Running Evaluation ***** Num examples = 8010 Batch size = 64 Saving model checkpoint to ./results/checkpoint-1500 Configuration saved in ./results/checkpoint-1500/config.json Model weights saved in ./results/checkpoint-1500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-2000 Configuration saved in ./results/checkpoint-2000/config.json Model weights saved in ./results/checkpoint-2000/pytorch_model.bin ***** Running Evaluation ***** Num examples = 8010 Batch size = 64 Saving model checkpoint to ./results/checkpoint-2500 Configuration saved in ./results/checkpoint-2500/config.json Model weights saved in ./results/checkpoint-2500/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-3000 Configuration saved in ./results/checkpoint-3000/config.json Model weights saved in ./results/checkpoint-3000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-3500 Configuration saved in ./results/checkpoint-3500/config.json Model weights saved in ./results/checkpoint-3500/pytorch_model.bin ***** Running Evaluation ***** Num examples = 8010 Batch size = 64 Saving model checkpoint to ./results/checkpoint-4000 Configuration saved in ./results/checkpoint-4000/config.json Model weights saved in ./results/checkpoint-4000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-4500 Configuration saved in ./results/checkpoint-4500/config.json Model weights saved in ./results/checkpoint-4500/pytorch_model.bin ***** Running Evaluation ***** Num examples = 8010 Batch size = 64 Saving model checkpoint to ./results/checkpoint-5000 Configuration saved in ./results/checkpoint-5000/config.json Model weights saved in ./results/checkpoint-5000/pytorch_model.bin Saving model checkpoint to ./results/checkpoint-5500 Configuration saved in ./results/checkpoint-5500/config.json Model weights saved in ./results/checkpoint-5500/pytorch_model.bin ***** Running Evaluation ***** Num examples = 8010 Batch size = 64 Training completed. Do not forget to share your model on huggingface.co/models =) . TrainOutput(global_step=5845, training_loss=0.24160565000196316, metrics={&#39;train_runtime&#39;: 3288.5532, &#39;train_samples_per_second&#39;: 56.837, &#39;train_steps_per_second&#39;: 1.777, &#39;total_flos&#39;: 3842141563120800.0, &#39;train_loss&#39;: 0.24160565000196316, &#39;epoch&#39;: 5.0}) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; predict which we pass our test dataset, returns a prediction (logits) for each sample in the dataset, and also computes the metrics that we already passed to the Trainer. . trainer.predict(test_dataset) . ***** Running Prediction ***** Num examples = 8011 Batch size = 64 . . [126/126 00:38] PredictionOutput(predictions=array([[-2.8933556 , 7.270326 , -1.6885829 , -1.338966 , -2.3586602 ], [-1.2138728 , -1.8854287 , -2.3426878 , 7.971615 , -2.857426 ], [-1.9059055 , -1.641494 , -3.739841 , 0.8998544 , 6.444405 ], ..., [-1.78565 , 6.1445756 , -1.9356711 , 0.23239179, -3.9032187 ], [-0.16582216, -1.4176931 , 2.6004167 , 0.93503976, -1.9505943 ], [ 8.251636 , -2.1360373 , -2.0708601 , -1.7121732 , -1.773536 ]], dtype=float32), label_ids=array([1, 3, 4, ..., 3, 3, 0]), metrics={&#39;test_loss&#39;: 0.8449447154998779, &#39;test_accuracy&#39;: 0.8483335413806017, &#39;test_f1&#39;: 0.8483335413806017, &#39;test_precision&#39;: 0.8483335413806017, &#39;test_recall&#39;: 0.8483335413806017, &#39;test_runtime&#39;: 38.7147, &#39;test_samples_per_second&#39;: 206.924, &#39;test_steps_per_second&#39;: 3.255}) . . We get 0.84 across all metrics by finetuning a model that was pretrained on one dialect. To see where we are, we take a look at the baseline results in the MSDA paper. . . In this Github repository, i compared DziriBERT to other arabic bert models (Arabert and Arabic-Bert) for this task, and it was able to outperform both of them even though they are pretrained on a lot more data. What is surprising is that the version of Arabert i used is pretrained on a dataset that includes 60M of multi-dialect tweets. . &lt;/div&gt; .",
            "url": "https://issam9.github.io/ml-blog/2021/10/19/Finetune-DziriBERT.html",
            "relUrl": "/2021/10/19/Finetune-DziriBERT.html",
            "date": " â€¢ Oct 19, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Building a language model for Moroccan Darija using fastai",
            "content": "This is a small effort to build a darija language model, i use Moroccan Darija Wikipedia to train an AWD_LSTM model using fastai, it is a small dataset which means that this language model won&#39;t be perfect for language generation but it might be useful to finetune it on a task like text classification following the ULMFiT approach, where you train a language model on Wikipedia text like we do in this notebook to gain some knowledge about the language of your choice, then finetune it on domain-specific data using the same objective of your pretrained language model, in order to bridge the gap between the language used in wikipedia text and the language used in your dataset (e.g., formal language -&gt; informal language), and finally, finetune the language model on the task of your choice. . This model can be improved by: . Throwing more data at it of course | Some text preprocessing | Tuning the hyperparameters | I thought also about pretraining on arabic which might be a good idea given the similarities between arabic and darija | . . Let&#39;s start by upgrading fastai and installing SentencePiece to use for subword tokenization: . !pip install fastai -q --upgrade !pip install -q sentencepiece!=0.1.90,!=0.1.91 . import sys from gensim.corpora import WikiCorpus from fastai.text.all import * import torch as torch import pandas as pd import numpy as np . from google.colab import drive drive.mount(&#39;/content/drive/&#39;) . path = Path(&#39;/content/drive/MyDrive/ml/projects/darija/&#39;) dls_path = path/&#39;dls&#39; model_path = path/&#39;models&#39; spm_path = model_path/&#39;spm&#39; dls_path.mkdir(exist_ok=True, parents=True) model_path.mkdir(exist_ok=True, parents=True) spm_path.mkdir(exist_ok=True, parents=True) . This is how we can download The Moroccan Darija Wikipedia data, it&#39;s available in this link. . !wget https://dumps.wikimedia.org/arywiki/latest/arywiki-latest-pages-articles.xml.bz2 -O &#39;/content/drive/MyDrive/ml/projects/darija/arywiki-latest-pages-articles.xml.bz2&#39; . --2021-08-29 19:53:23-- https://dumps.wikimedia.org/arywiki/latest/arywiki-latest-pages-articles.xml.bz2 Resolving dumps.wikimedia.org (dumps.wikimedia.org)... 208.80.154.7, 2620:0:861:1:208:80:154:7 Connecting to dumps.wikimedia.org (dumps.wikimedia.org)|208.80.154.7|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 4178623 (4.0M) [application/octet-stream] Saving to: â€˜/content/drive/MyDrive/ml/projects/darija/arywiki-latest-pages-articles.xml.bz2â€™ /content/drive/MyDr 100%[===================&gt;] 3.98M 5.23MB/s in 0.8s 2021-08-29 19:53:24 (5.23 MB/s) - â€˜/content/drive/MyDrive/ml/projects/darija/arywiki-latest-pages-articles.xml.bz2â€™ saved [4178623/4178623] . We make use of WikiCorpus from gensim to convert the XML file we downloaded to a text corpus. . def make_corpus(in_f, out_f): &quot;&quot;&quot;Convert Wikipedia xml dump file to text corpus&quot;&quot;&quot; output = open(out_f, &#39;w&#39;) wiki = WikiCorpus(in_f) for i, text in enumerate(wiki.get_texts()): output.write(bytes(&#39; &#39;.join(text), &#39;utf-8&#39;).decode(&#39;utf-8&#39;) + &#39; n&#39;) if (i % 1000 == 0): print(&#39;Processed &#39; + str(i) + &#39; articles&#39;) output.close() print(&#39;Processing complete!&#39;) make_corpus(f&#39;{path}/arywiki-latest-pages-articles.xml.bz2&#39;, f&#39;{path}/wiki_darija.txt&#39;) . Processed 0 articles Processed 1000 articles Processed 2000 articles Processed 3000 articles Processing complete! . path.ls() . (#10) [Path(&#39;/content/drive/MyDrive/ml/projects/darija/arwiki.xml.bz2&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/dls&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/models&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/arwiki-latest-pages-articles.xml.bz2&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/wiki_arabic.txt&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/Untitled0.ipynb&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/Copie de darija_lm.ipynb&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/arywiki-latest-pages-articles.xml.bz2&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/wiki_darija.txt&#39;),Path(&#39;/content/drive/MyDrive/ml/projects/darija/darija_lm.ipynb&#39;)] . Now we load our text data as a pandas dataframe, and we take a look at it using the most advanced EDA technique ğŸ˜„, we can see that there are words from other languages that will most likely disappear due to their low frequency, and we can tell fastai the minimum word frequency (by default itâ€™s 3) we can tolerate using fastai DataBlocks that we discuss below. . df = pd.read_csv(path/&#39;wiki_darija.txt&#39;, header=None, names=[&#39;text&#39;]) df.head() . text . 0 Ø¢Ø¨Ø·Ø­ Ø¬Ù…Ø§Ø¹Ø© ØªØ±Ø§Ø¨ÙŠØ© Ù‚Ø±ÙˆÙŠØ© ÙƒØ§ÙŠÙ†Ø© Ø¥Ù‚Ù„ÙŠÙ… Ø¹Ù…Ø§Ù„Ø© Ø·Ø§Ù† Ø·Ø§Ù† Ø¬Ù‡Ø© İ£Ù„Ù…ÙŠÙ… ÙˆØ§Ø¯ Ù†ÙˆÙ† Ø³Ø§ÙƒÙ†ÙŠÙ† ÙÙŠÙ‡Ø§ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ù„Ø¥Ø­ØµØ§Ø¡ Ù„Ø¹Ø§Ù… ØªØ¹Ù„ÙŠÙ… Ù†Ø³Ø¨Ø© Ù„Ø£Ù…ÙŠØ© Ø§Ø³ Ù…Ø§ ÙƒØ§ÙŠØ¹Ø±ÙÙˆØ´ ÙŠÙ‚Ø±Ø§Ùˆ ÙˆÙ„Ø§ ÙŠÙƒØªØ¨Ùˆ Ù†Ø³Ø¨Ø© ÙƒØ§Ù† Ù‚Ø§Ø±ÙŠÙŠÙ† ÙÙˆÙ‚ Ø§Ù†ÙˆÙŠ ØªØ§Ù†ÙˆÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù‚ØªØµØ§Ø¯ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø´ÙŠØ·ÙŠÙ† ÙŠÙ‚Ø¯Ø±Ùˆ ÙŠØ®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ù„Ø¨Ø·Ø§Ù„Ø© Ø§Ø³ Ù…Ø§ Ø®Ø¯Ø§Ù…ÙŠÙ†Ø´ ØªØ§ÙŠÙ‚Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ Ø®Ø¯Ù…Ø© Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙˆÙ„Ø© ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙÙŠ Ù„Ù‚Ø·Ø§Ø¹ Ù„Ø®Ø§Øµ ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ø¹ÙŠÙˆÙ† Ù„ÙƒÙ„Ø§Ù… ØªØµÙ†ÙŠÙ Ø¬Ù‡Ø© İ£Ù„Ù…ÙŠÙ… ÙˆØ§Ø¯ Ù†ÙˆÙ† | . 1 Ø¢Ø³ÙÙŠ Ø¨Ø§Ù„Ø£Ù…Ø§Ø²ÙŠØºÙŠØ© â´°âµ™â´¼âµ‰ Ù‡ÙŠ Ù…Ø¯ÙŠÙ†Ø© Ù…ØºØ±Ø¨ÙŠØ© Ø¬Ø§Øª Ø¥Ù‚Ù„ÙŠÙ… Ø¢Ø³ÙÙŠ Ø¬Ù‡Ø© Ù…Ø±Ø§ÙƒØ´ Ø¢Ø³ÙÙŠ Ø¢Ø³ÙÙŠ Ù…Ø¹Ø±ÙˆÙØ© Ø¨Ø§Ù„ÙØ®Ø§Ø± ÙˆØ§Ù„Ø­ÙˆØª ÙˆØ®ØµÙˆØµØ§ Ø§Ù„Ø³Ø±Ø¯ÙŠÙ† ÙˆÙ…ÙƒÙ†ÙŠÙŠÙ† Ø¹Ù„ÙŠÙ‡Ø§ Ø­Ø§Ø¶Ø±Ø© Ø§Ù„Ù…Ø­ÙŠØ· Ø§Ù„Ø­Ø·Ø© Ø¯ÙŠØ§Ù„ Ø¢Ø³ÙÙŠ Ø¬Ø§Øª ÙƒØ§Ø·Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ÙŠØ· Ø§Ù„Ø£Ø·Ù„Ø³ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© ÙˆØ§Ù„ØµÙˆÙŠØ±Ø© ÙÙŠ Ø¢Ø³ÙÙŠ ÙƒØ§ÙŠÙ† Ø¨Ø²Ø§Ù Ø¯Ø§Ù„Ø¨Ù†ÙŠ Ù„ÙŠ Ù‚Ø¯ÙŠÙ… ÙˆØªØ§Ø±ÙŠØ®ÙŠ ÙˆÙ‡ÙŠ Ù…Ù† Ø¨ÙŠÙ† Ø§Ù„Ù…Ø¯ÙˆÙ† Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© ÙÙŠ Ø§Ù„Ù…ØºØ±Ø¨ Ø³Ø§ÙƒÙ†ÙŠÙ† ÙÙŠÙ‡Ø§ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ù„Ø¥Ø­ØµØ§Ø¡ Ù„Ø¹Ø§Ù… ØªØ¹Ù„ÙŠÙ… Ù†Ø³Ø¨Ø© Ù„Ø£Ù…ÙŠØ© Ø§Ø³ Ù…Ø§ ÙƒØ§ÙŠØ¹Ø±ÙÙˆØ´ ÙŠÙ‚Ø±Ø§Ùˆ ÙˆÙ„Ø§ ÙŠÙƒØªØ¨Ùˆ Ù†Ø³Ø¨Ø© ÙƒØ§Ù† Ù‚Ø§Ø±ÙŠÙŠÙ† ÙÙˆÙ‚ Ø§Ù†ÙˆÙŠ ØªØ§Ù†ÙˆÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù‚ØªØµØ§Ø¯ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø´ÙŠØ·ÙŠÙ† ÙŠÙ‚Ø¯Ø±Ùˆ ÙŠØ®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ù„Ø¨Ø·Ø§Ù„Ø© Ø§Ø³ Ù…Ø§ Ø®Ø¯Ø§Ù…ÙŠÙ†Ø´ ØªØ§ÙŠÙ‚Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ Ø®Ø¯Ù…Ø© Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙˆÙ„Ø© ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙÙŠ Ù„Ù‚Ø·Ø§Ø¹ Ù„Ø®Ø§Øµ ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯... | . 2 Ø¢Ù„Ø¨Ø±Ø®Øª Ø¯ÙˆØ±Ø± Ø¨Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ© albrecht dÃ¼rer Ù…Ø§ÙŠ Ø£Ø¨Ø±ÙŠÙ„ Ø±Ø³Ø§Ù… ØµØ§Ù†Ø¹ Ø·Ø¨Ø§Ø¹Ø© ÙˆÙ… ÙƒØ§ÙŠÙ†ØªØ§Ù…ÙŠ Ù„Ø¹ØµØ± Ø§Ù„Ù†Ù‡Ø¶Ø© Ø§Ù„Ø£Ù„Ù…Ø§Ù†ÙŠØ© ØªØ²Ø§Ø¯ ÙÙŠ Ù†ÙˆØ±Ù…Ø¨Ø±İ£ Ø¯ÙˆØ±Ø± Ø£Ø³Ø³ Ù„Ù„Ø³Ù…Ø¹Ø© ÙˆØ§Ù„ØªØ£Ø«ÙŠØ± Ø¯ÙŠØ§Ù„Ùˆ Ø¹Ø¨Ø± Ø£ÙˆØ±ÙˆØ¨Ø§ ÙØ§Ù„ÙˆÙ‚Øª Ø§Ù„Ù„ÙŠ Ù…Ø§Ø²Ø§Ù„Ø§ ÙØ§Ù„Ø¹Ø´Ø±ÙŠÙ†Ø§Øª Ù…Ù† Ø¹Ù…Ø±Ùˆ Ù†ØªÙŠØ¬Ø© Ù„Ø¬ÙˆØ¯Ø© Ù†Ù‚ÙˆØ´Ø§ØªÙˆ Ø§Ù„Ø®Ø´Ø¨ÙŠØ© ÙƒØ§Ù† ÙØ§ØªØµØ§Ù„ Ù…Ø¹ Ø£ÙƒØ¨Ø± Ø§Ù„ÙÙ†Ø§Ù†ÙŠÙ† Ø§Ù„Ø¥ÙŠØ·Ø§Ù„ÙŠÙŠÙ† ÙØ§Ù„Ø¹ØµØ± Ø¯ÙŠØ§Ù„Ùˆ Ø¨Ù…Ø§ ÙÙŠÙ‡Ù… Ø±ÙØ§Ø¦ÙŠÙ„ Ø¬ÙŠÙˆÚ¤Ø§Ù†ÙŠ Ø¨ÙŠÙ„ÙŠÙ†ÙŠ ÙˆÙ„ÙŠÙˆÙ†Ø§Ø±Ø¯Ùˆ Ø¯Ø§ Ú¤ÙŠÙ†ØªØ´ÙŠ ÙˆØ§Ø¨ØªØ¯Ø§Ø¡ Ù…Ù† ÙƒØ§Ù† ÙƒÙŠ Ø§Ø®Ø¯ Ø§Ù„Ø¯Ø¹Ù… Ù…Ù† Ø¹Ù†Ø¯ Ø§Ù„Ø¥Ù…Ø¨Ø±Ø§Ø·ÙˆØ± Ù…Ø§ÙƒØ³ÙŠÙ…ÙŠÙ„ÙŠØ§Ù† Ø§Ù„Ø£ÙˆÙ„ ØªÙ… ØªØ´ÙŠÙŠØ¹ Ø¯ÙˆØ±Ø± ÙØ§Ù„ÙƒÙ†ÙŠØ³ØªÙŠÙ† Ø§Ù„Ù„ÙˆØ«Ø±ÙŠØ© ÙˆØ§Ù„Ø£Ø³Ù‚ÙÙŠØ© Ø¨Ø¬ÙˆØ¬ ÙƒØªØ´Ù…Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø£Ø¹Ù…Ø§Ù„ Ø¯ÙˆØ±Ø± Ø§Ù„ÙˆØ§Ø³Ø¹Ø© Ø§Ù„Ù†Ù‚ÙˆØ´ Ø£Ø³Ù„ÙˆØ¨Ùˆ Ø§Ù„Ù…ÙØ¶Ù„ ÙÙŠ Ø·Ø¨Ø¹Ø§ØªÙˆ Ø§Ù„Ø£Ø®ÙŠØ±Ø© Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„ÙÙ†ÙŠØ© ÙØ§Ù„ÙƒÙ†Ø§Ø¦Ø³ altarpieces Ù¾ÙˆØ±ØªØ±ÙŠÙ‡Ø§Øª Ù¾ÙˆØ±ØªØ±ÙŠÙ‡Ø§Øª Ø°Ø§ØªÙŠØ© Ù„ÙˆØ­Ø§Øª Ù…Ø§Ø¦ÙŠØ© ÙˆÙƒ Ø§Ù„Ù†Ù‚ÙˆØ´ Ø§Ù„Ø®... | . 3 Ø¢Ù…ØªØ¯ÙŠ Ø¬Ù…Ø§Ø¹Ø© ØªØ±Ø§Ø¨ÙŠØ© Ù‚Ø±ÙˆÙŠØ© ÙƒØ§ÙŠÙ†Ø© Ø¥Ù‚Ù„ÙŠÙ… Ø¹Ù…Ø§Ù„Ø© İ£Ù„Ù…ÙŠÙ… Ø¬Ù‡Ø© İ£Ù„Ù…ÙŠÙ… ÙˆØ§Ø¯ Ù†ÙˆÙ† Ø³Ø§ÙƒÙ†ÙŠÙ† ÙÙŠÙ‡Ø§ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ù„Ø¥Ø­ØµØ§Ø¡ Ù„Ø¹Ø§Ù… ØªØ¹Ù„ÙŠÙ… Ù†Ø³Ø¨Ø© Ù„Ø£Ù…ÙŠØ© Ø§Ø³ Ù…Ø§ ÙƒØ§ÙŠØ¹Ø±ÙÙˆØ´ ÙŠÙ‚Ø±Ø§Ùˆ ÙˆÙ„Ø§ ÙŠÙƒØªØ¨Ùˆ Ù†Ø³Ø¨Ø© ÙƒØ§Ù† Ù‚Ø§Ø±ÙŠÙŠÙ† ÙÙˆÙ‚ Ø§Ù†ÙˆÙŠ ØªØ§Ù†ÙˆÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù‚ØªØµØ§Ø¯ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø´ÙŠØ·ÙŠÙ† ÙŠÙ‚Ø¯Ø±Ùˆ ÙŠØ®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ù„Ø¨Ø·Ø§Ù„Ø© Ø§Ø³ Ù…Ø§ Ø®Ø¯Ø§Ù…ÙŠÙ†Ø´ ØªØ§ÙŠÙ‚Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ Ø®Ø¯Ù…Ø© Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙˆÙ„Ø© ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙÙŠ Ù„Ù‚Ø·Ø§Ø¹ Ù„Ø®Ø§Øµ ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ø¹ÙŠÙˆÙ† Ù„ÙƒÙ„Ø§Ù… ØªØµÙ†ÙŠÙ Ø¬Ù‡Ø© İ£Ù„Ù…ÙŠÙ… ÙˆØ§Ø¯ Ù†ÙˆÙ† | . 4 Ø¢Ù†Ùİ£ Ø¬Ù…Ø§Ø¹Ø© ØªØ±Ø§Ø¨ÙŠØ© Ù‚Ø±ÙˆÙŠØ© ÙƒØ§ÙŠÙ†Ø© Ø¥Ù‚Ù„ÙŠÙ… Ø¹Ù…Ø§Ù„Ø© Ø³ÙŠØ¯ÙŠ Ø¥ÙŠÙÙ†ÙŠ Ø¬Ù‡Ø© İ£Ù„Ù…ÙŠÙ… ÙˆØ§Ø¯ Ù†ÙˆÙ† Ø³Ø§ÙƒÙ†ÙŠÙ† ÙÙŠÙ‡Ø§ ÙˆØ§Ø­Ø¯ Ø¹Ù„Ù‰ Ø­Ø³Ø¨ Ù„Ø¥Ø­ØµØ§Ø¡ Ù„Ø¹Ø§Ù… ØªØ¹Ù„ÙŠÙ… Ù†Ø³Ø¨Ø© Ù„Ø£Ù…ÙŠØ© Ø§Ø³ Ù…Ø§ ÙƒØ§ÙŠØ¹Ø±ÙÙˆØ´ ÙŠÙ‚Ø±Ø§Ùˆ ÙˆÙ„Ø§ ÙŠÙƒØªØ¨Ùˆ Ù†Ø³Ø¨Ø© ÙƒØ§Ù† Ù‚Ø§Ø±ÙŠÙŠÙ† ÙÙˆÙ‚ Ø§Ù†ÙˆÙŠ ØªØ§Ù†ÙˆÙŠ Ø¬Ø§Ù…Ø¹Ø© Ø§Ù‚ØªØµØ§Ø¯ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø´ÙŠØ·ÙŠÙ† ÙŠÙ‚Ø¯Ø±Ùˆ ÙŠØ®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ù„Ø¨Ø·Ø§Ù„Ø© Ø§Ø³ Ù…Ø§ Ø®Ø¯Ø§Ù…ÙŠÙ†Ø´ ØªØ§ÙŠÙ‚Ù„Ø¨Ùˆ Ø¹Ù„Ù‰ Ø®Ø¯Ù…Ø© Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙˆÙ„Ø© ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ù†Ø³Ø¨Ø© Ø§Ø³ Ø§Ù„Ù„ÙŠ Ø®Ø¯Ø§Ù…ÙŠÙ† ÙÙŠ Ù„Ù‚Ø·Ø§Ø¹ Ù„Ø®Ø§Øµ ÙˆÙ„Ø§ Ù„Ø¹Ø§Ø·Ù„ÙŠÙ† Ø§Ù„Ù„ÙŠ Ø³Ø¨Ù‚ Ù„ÙŠÙ‡ÙˆÙ… Ø®Ø¯Ù…Ùˆ Ø¹ÙŠÙˆÙ† Ù„ÙƒÙ„Ø§Ù… ØªØµÙ†ÙŠÙ Ø¬Ù‡Ø© İ£Ù„Ù…ÙŠÙ… ÙˆØ§Ø¯ Ù†ÙˆÙ† | . Subword tokenization refers to constructing our vocabulary using the most frequently occurring groups of letters, for instance, the word â€œtransformerâ€ could be split into â€œtransâ€ and â€œformerâ€. I find it better to use subword tokenization with a relatively smaller vocabulary size in the case of a small dataset, to avoid the p&gt;&gt;n problem (where the number of features exceeds the number of training examples), and also because if we decide to use words as our tokens, we are going to have a lot of words that appear only a few times throughout the corpus, and the model wonâ€™t be given a decent chance to learn about them. . I use a maximum vocabilary size of 1000 specified by the max_vocab_sz parameter, but you can use less or more, its another hyperparamter you can tune based on the metric you care about. . The data block API is provided by fastai to customize the creation of our dataloaders, blocks parameter is used to specify the type of our independent and dependent variables, when TextBlock is passed, fastai takes care of preprocessing for us, we just need to pass it our subword tokenizer since it uses word tokenization by default, we also tell fastai that we are building this for language modeling with is_lm and that our text is in a dataframe. . And finally we create our dataloaders, it&#39;s a dataloader with s because it includes the training and validation dataloaders, the validation set is 10% of our data as we specify in our RandomSplitter. . bs=128 tok = SubwordTokenizer(cache_dir=spm_path, max_vocab_sz=1000) dls_lm = DataBlock(blocks=TextBlock.from_df(&#39;text&#39;, is_lm=True, tok=tok), splitter=RandomSplitter(0.1, seed=42), get_x=ColReader(&#39;text&#39;) ).dataloaders(df, bs=bs) . /usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify &#39;dtype=object&#39; when creating the ndarray return array(a, dtype, copy=False, order=order) . We save our dataloader since we can&#39;t afford to create it each time because of our huge dataset ğŸ˜…. . torch.save(dls_lm, dls_path/&#39;dls_lm.pkl&#39;) . This is how our preprocessed text looks like, spaces in the original text are replaced by â–, xxbos is a special token added by fastai to signify the beginning of a sentence, fastai also adds other special tokens to make learning easier for the model, we can see them when we check our vocab below. . dls_lm.show_batch(max_n=6) . text text_ . 0 â–xxbos â–Ø¨Ù†ÙŠ â–Ù…Ù† Øµ ÙˆØ± â–Ø¬Ù…Ø§Ø¹ Ø© â–ØªØ±Ø§Ø¨ÙŠØ© â–Ù‚Ø±Ùˆ ÙŠØ© â–ÙƒØ§ÙŠÙ† Ø© â–Ø¥Ù‚Ù„ÙŠÙ… â–Ø¹Ù…Ø§Ù„ Ø© â–Ø¥Ù‚Ù„ÙŠÙ… â– Ø´ Ù Ø´ Ø§ ÙˆÙ† â–Ø¬Ù‡ Ø© â–Ø·Ù†Ø¬ Ø© â–ØªØ·ÙˆØ§Ù† â–Ø§Ù„Ø­Ø³ÙŠÙ…Ø© â–Ø³Ø§ÙƒÙ†ÙŠÙ† â–ÙÙŠÙ‡Ø§ â–ÙˆØ§Ø­Ø¯ â–Ø¹Ù„ Ù‰ â–Ø­Ø³Ø¨ â–Ù„ Ø¥Ø­ØµØ§Ø¡ â–Ù„Ø¹Ø§Ù… â–ØªØ¹Ù„ÙŠÙ… â–Ù†Ø³Ø¨Ø© â–Ù„Ø£Ù…ÙŠ Ø© â–Ø§Ø³ â–Ù…Ø§ â–ÙƒØ§ÙŠØ¹Ø±ÙÙˆ Ø´ â–ÙŠÙ‚Ø±Ø§ Ùˆ â–ÙˆÙ„Ø§ â–ÙŠÙƒØªØ¨ Ùˆ â–Ù†Ø³Ø¨Ø© â–ÙƒØ§Ù† â– Ù‚Ø§Ø±ÙŠÙŠÙ† â–ÙÙˆÙ‚ â– Ø§Ù†ÙˆÙŠ â–ØªØ§Ù†ÙˆÙŠ â–Ø¬Ø§Ù…Ø¹ Ø© â–Ø§Ù‚ØªØµØ§Ø¯ â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø´ÙŠØ· ÙŠÙ† â– ÙŠÙ‚Ø¯Ø±Ùˆ â– ÙŠØ®Ø¯Ù…Ùˆ â–Ù†Ø³Ø¨Ø© â–Ù„Ø¨Ø·Ø§Ù„ Ø© | â–Ø¨Ù†ÙŠ â–Ù…Ù† Øµ ÙˆØ± â–Ø¬Ù…Ø§Ø¹ Ø© â–ØªØ±Ø§Ø¨ÙŠØ© â–Ù‚Ø±Ùˆ ÙŠØ© â–ÙƒØ§ÙŠÙ† Ø© â–Ø¥Ù‚Ù„ÙŠÙ… â–Ø¹Ù…Ø§Ù„ Ø© â–Ø¥Ù‚Ù„ÙŠÙ… â– Ø´ Ù Ø´ Ø§ ÙˆÙ† â–Ø¬Ù‡ Ø© â–Ø·Ù†Ø¬ Ø© â–ØªØ·ÙˆØ§Ù† â–Ø§Ù„Ø­Ø³ÙŠÙ…Ø© â–Ø³Ø§ÙƒÙ†ÙŠÙ† â–ÙÙŠÙ‡Ø§ â–ÙˆØ§Ø­Ø¯ â–Ø¹Ù„ Ù‰ â–Ø­Ø³Ø¨ â–Ù„ Ø¥Ø­ØµØ§Ø¡ â–Ù„Ø¹Ø§Ù… â–ØªØ¹Ù„ÙŠÙ… â–Ù†Ø³Ø¨Ø© â–Ù„Ø£Ù…ÙŠ Ø© â–Ø§Ø³ â–Ù…Ø§ â–ÙƒØ§ÙŠØ¹Ø±ÙÙˆ Ø´ â–ÙŠÙ‚Ø±Ø§ Ùˆ â–ÙˆÙ„Ø§ â–ÙŠÙƒØªØ¨ Ùˆ â–Ù†Ø³Ø¨Ø© â–ÙƒØ§Ù† â– Ù‚Ø§Ø±ÙŠÙŠÙ† â–ÙÙˆÙ‚ â– Ø§Ù†ÙˆÙŠ â–ØªØ§Ù†ÙˆÙŠ â–Ø¬Ø§Ù…Ø¹ Ø© â–Ø§Ù‚ØªØµØ§Ø¯ â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø´ÙŠØ· ÙŠÙ† â– ÙŠÙ‚Ø¯Ø±Ùˆ â– ÙŠØ®Ø¯Ù…Ùˆ â–Ù†Ø³Ø¨Ø© â–Ù„Ø¨Ø·Ø§Ù„ Ø© â–Ø§Ø³ | . 1 â–Ø¯ÙŠØ§Ù„ Ùƒ Ù… â–Ù„Ù„ Ù† Ø³ Ø§ â– ÙˆÙ† â–Ø¯ÙŠØ§Ù„Ùˆ â– ÙˆÙ† â–Ø³ Ù† â–Ø¯ÙŠØ§Ù„ Ù‡Ù… â– ÙˆÙ† â–Ø³ Ù†Øª â–Ø¯ÙŠØ§Ù„ Ù‡Ù… â–Ù„Ù„ Ù† Ø³ Ø§ â–Ø§Ù„Ù… Ù„ Ùƒ ÙŠØ© â–Ù„Ù„ Ù† Ø³ Ø§ â–Ù„ Ù„Ùˆ Ø­Ø¯ Ø© â–Øª Ù† â– ÙŠÙ† Ùˆ â–Ø¯ÙŠØ§Ù„ ØªÙŠ â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª Ù†Ø§ â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª Ùƒ â– Ø° ÙƒØ± â–Øª Ù† â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª Ùƒ Ù… â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª ÙƒÙŠ â–Ù… | Ùƒ Ù… â–Ù„Ù„ Ù† Ø³ Ø§ â– ÙˆÙ† â–Ø¯ÙŠØ§Ù„Ùˆ â– ÙˆÙ† â–Ø³ Ù† â–Ø¯ÙŠØ§Ù„ Ù‡Ù… â– ÙˆÙ† â–Ø³ Ù†Øª â–Ø¯ÙŠØ§Ù„ Ù‡Ù… â–Ù„Ù„ Ù† Ø³ Ø§ â–Ø§Ù„Ù… Ù„ Ùƒ ÙŠØ© â–Ù„Ù„ Ù† Ø³ Ø§ â–Ù„ Ù„Ùˆ Ø­Ø¯ Ø© â–Øª Ù† â– ÙŠÙ† Ùˆ â–Ø¯ÙŠØ§Ù„ ØªÙŠ â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª Ù†Ø§ â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª Ùƒ â– Ø° ÙƒØ± â–Øª Ù† â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª Ùƒ Ù… â–Øª Ù† â–Ø¯ÙŠØ§Ù„ Øª ÙƒÙŠ â–Ù… Ø¤ | . 2 â–Ù…Ù† â– ÙˆÙ â–Ø¹ Ø³ Ù â–Ø® Ù„ â–Ø¤Ù„Ø§ â–Ù„Ø® Ø´ Ø¨ â–Ù… Øº Ù„Ù Ø© â–Ù… Ø²ÙŠ Ø§Ù† â– Ø·Ø± Ù â–Ù„ Ø¯ â–Ø¯ÙŠØ§Ù„ â–Ø¨ İ£Ø± Ø© â–Ø¤Ù„Ø§ â–Øª ÙˆØ± â–Ù„Ùƒ ÙˆØ± Ø© â–Ø¨Ø§Ø´ â–ÙƒÙŠ Ù„Ø¹Ø¨ Ùˆ â–Ø¬Ø§ Ù„ â–ÙƒØª ÙƒÙˆÙ† â–Ø¹ Ø§Ø¯ Ø© â–Øª Ù‚Ù„ â–Ù…Ù† â–ÙƒÙŠ Ù„Ø¹Ø¨ Ùˆ â– Ø¨ÙŠ Ù‡Ø§ â–Ø³Ø§ â–Ùˆ Ø¹ Ø¨ Ø© â–Ù…Ø± Ø§Øª â–ÙƒØª ÙƒÙˆÙ† â–Ø® Ø· ÙŠØ± Ø© â– Ùƒ ÙŠÙ‚Ø¯Ø±Ùˆ â–ÙŠÙˆ Ù‚ | â– ÙˆÙ â–Ø¹ Ø³ Ù â–Ø® Ù„ â–Ø¤Ù„Ø§ â–Ù„Ø® Ø´ Ø¨ â–Ù… Øº Ù„Ù Ø© â–Ù… Ø²ÙŠ Ø§Ù† â– Ø·Ø± Ù â–Ù„ Ø¯ â–Ø¯ÙŠØ§Ù„ â–Ø¨ İ£Ø± Ø© â–Ø¤Ù„Ø§ â–Øª ÙˆØ± â–Ù„Ùƒ ÙˆØ± Ø© â–Ø¨Ø§Ø´ â–ÙƒÙŠ Ù„Ø¹Ø¨ Ùˆ â–Ø¬Ø§ Ù„ â–ÙƒØª ÙƒÙˆÙ† â–Ø¹ Ø§Ø¯ Ø© â–Øª Ù‚Ù„ â–Ù…Ù† â–ÙƒÙŠ Ù„Ø¹Ø¨ Ùˆ â– Ø¨ÙŠ Ù‡Ø§ â–Ø³Ø§ â–Ùˆ Ø¹ Ø¨ Ø© â–Ù…Ø± Ø§Øª â–ÙƒØª ÙƒÙˆÙ† â–Ø® Ø· ÙŠØ± Ø© â– Ùƒ ÙŠÙ‚Ø¯Ø±Ùˆ â–ÙŠÙˆ Ù‚ Ø¹ | . 3 Ø§Øª â–ÙˆØ§Ù„ Ø·Ø§ ÙƒØ³ ÙŠØ§Øª â–Ù„ÙŠ â–Ø¹Ù†Ø¯ Ù‡Ù… â–Ø§Ù„Ø­ Ù‚ â–ÙŠ Ø¯Ø®Ù„ Ùˆ â–Ù„ÙŠ Ù‡Ø§ â– Ø§ Øµ Ù„Ø§ Ø­ â–Ø§Ù„ Ù‚Øµ Ø¨ Ø© â–Ù… Ø´ Ø±Ùˆ Ø¹ â– Ø§ Øµ Ù„Ø§ Ø­ â–Ù‚ ØµØ¨ Ø© â–Ø£ İ£ Ø§Ø¯ ÙŠØ± â–Ø£Ùˆ Ù Ù„Ø§ â–Ù‡Ùˆ â–Ù… Ø´ Ø±Ùˆ Ø¹ â–Ø¥ Ù† Ø·Ù„ Ù‚ â–Ø¹Ø§Ù… â–ÙˆØª Ø§ÙŠ Ù‡Ø¯ Ù â–Ø¨Ø§Ø´ â–ÙŠ Øµ Ù„ Ø­ â– Ø§ÙˆÙŠ Ø£ Ù‡ Ù„ â–Ø§Ù„Ø³ Ùˆ Ø§Ø± â–Ø¯ Ø§Ù„ | â–ÙˆØ§Ù„ Ø·Ø§ ÙƒØ³ ÙŠØ§Øª â–Ù„ÙŠ â–Ø¹Ù†Ø¯ Ù‡Ù… â–Ø§Ù„Ø­ Ù‚ â–ÙŠ Ø¯Ø®Ù„ Ùˆ â–Ù„ÙŠ Ù‡Ø§ â– Ø§ Øµ Ù„Ø§ Ø­ â–Ø§Ù„ Ù‚Øµ Ø¨ Ø© â–Ù… Ø´ Ø±Ùˆ Ø¹ â– Ø§ Øµ Ù„Ø§ Ø­ â–Ù‚ ØµØ¨ Ø© â–Ø£ İ£ Ø§Ø¯ ÙŠØ± â–Ø£Ùˆ Ù Ù„Ø§ â–Ù‡Ùˆ â–Ù… Ø´ Ø±Ùˆ Ø¹ â–Ø¥ Ù† Ø·Ù„ Ù‚ â–Ø¹Ø§Ù… â–ÙˆØª Ø§ÙŠ Ù‡Ø¯ Ù â–Ø¨Ø§Ø´ â–ÙŠ Øµ Ù„ Ø­ â– Ø§ÙˆÙŠ Ø£ Ù‡ Ù„ â–Ø§Ù„Ø³ Ùˆ Ø§Ø± â–Ø¯ Ø§Ù„ Ù‚Øµ | . 4 â–Ù„Ø¹Ø§Ø· Ù„ÙŠÙ† â–Ø§Ù„Ù„ÙŠ â–Ø³Ø¨Ù‚ â–Ù„ ÙŠÙ‡ÙˆÙ… â–Ø®Ø¯Ù…Ùˆ â–Ø¹ÙŠÙˆÙ† â–Ù„ÙƒÙ„Ø§Ù… â–ØªØµÙ†ÙŠÙ â–Ø¬Ù‡ Ø© â–Ù…Ø±Ø§ÙƒØ´ â–Ø¢Ø³ÙÙŠ â–xxbos â–Ù„Ø­ Ø§Ø¬ â–Ù„Ø­ Ø³ ÙŠÙ† â–Ø§Ù„Øª ÙˆÙ„ Ø§Ù„ÙŠ â–ØªÙˆÙ„Ø¯ â–Øª ÙˆÙ„ Ø§Ù„ â–Ù…Ø§Øª â–Ù†Ù‡Ø§Ø± â–Ø¯ Ø¬ Ù† Ø¨Ø± â–Ù Ù… Ùƒ Ù†Ø§Ø³ â–ÙƒØ§Ù† â–Ù Ù† Ø§Ù† â–Ù… ØºØ±Ø¨ ÙŠ â–Ø¯ÙŠØ§Ù„ â–Ù„Ù… Ù„ Ø­ ÙˆÙ† â–Ø§Ù„Ø­ Ø³ ÙŠÙ† â–Ø§Ù„Øª ÙˆÙ„ Ø§Ù„ÙŠ â–ØªÙˆÙ„Ø¯ â–Ù Øª ÙˆÙ„ Ø§Ù„ â–Ø­ Ø¯Ø§ â–Ù…ÙƒÙ†Ø§Ø³ â–Ùˆ ÙƒØ§Ù† Ùˆ â–ØªÙŠ Ù‡Ø¶Ø± Ùˆ â–Ø¨Ø§Ù„ Ø´ Ù„ | Ù„ÙŠÙ† â–Ø§Ù„Ù„ÙŠ â–Ø³Ø¨Ù‚ â–Ù„ ÙŠÙ‡ÙˆÙ… â–Ø®Ø¯Ù…Ùˆ â–Ø¹ÙŠÙˆÙ† â–Ù„ÙƒÙ„Ø§Ù… â–ØªØµÙ†ÙŠÙ â–Ø¬Ù‡ Ø© â–Ù…Ø±Ø§ÙƒØ´ â–Ø¢Ø³ÙÙŠ â–xxbos â–Ù„Ø­ Ø§Ø¬ â–Ù„Ø­ Ø³ ÙŠÙ† â–Ø§Ù„Øª ÙˆÙ„ Ø§Ù„ÙŠ â–ØªÙˆÙ„Ø¯ â–Øª ÙˆÙ„ Ø§Ù„ â–Ù…Ø§Øª â–Ù†Ù‡Ø§Ø± â–Ø¯ Ø¬ Ù† Ø¨Ø± â–Ù Ù… Ùƒ Ù†Ø§Ø³ â–ÙƒØ§Ù† â–Ù Ù† Ø§Ù† â–Ù… ØºØ±Ø¨ ÙŠ â–Ø¯ÙŠØ§Ù„ â–Ù„Ù… Ù„ Ø­ ÙˆÙ† â–Ø§Ù„Ø­ Ø³ ÙŠÙ† â–Ø§Ù„Øª ÙˆÙ„ Ø§Ù„ÙŠ â–ØªÙˆÙ„Ø¯ â–Ù Øª ÙˆÙ„ Ø§Ù„ â–Ø­ Ø¯Ø§ â–Ù…ÙƒÙ†Ø§Ø³ â–Ùˆ ÙƒØ§Ù† Ùˆ â–ØªÙŠ Ù‡Ø¶Ø± Ùˆ â–Ø¨Ø§Ù„ Ø´ Ù„ Ø­ | . 5 â–ÙƒØ§ÙŠØ¹Ø±ÙÙˆ Ø´ â–ÙŠÙ‚Ø±Ø§ Ùˆ â–ÙˆÙ„Ø§ â–ÙŠÙƒØªØ¨ Ùˆ â–Ù†Ø³Ø¨Ø© â–ÙƒØ§Ù† â– Ù‚Ø§Ø±ÙŠÙŠÙ† â–ÙÙˆÙ‚ â– Ø§Ù†ÙˆÙŠ â–ØªØ§Ù†ÙˆÙŠ â–Ø¬Ø§Ù…Ø¹ Ø© â–Ø§Ù‚ØªØµØ§Ø¯ â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø´ÙŠØ· ÙŠÙ† â– ÙŠÙ‚Ø¯Ø±Ùˆ â– ÙŠØ®Ø¯Ù…Ùˆ â–Ù†Ø³Ø¨Ø© â–Ù„Ø¨Ø·Ø§Ù„ Ø© â–Ø§Ø³ â–Ù…Ø§ â–Ø®Ø¯Ø§Ù…ÙŠÙ† Ø´ â–ØªØ§ÙŠÙ‚ Ù„Ø¨Ùˆ â–Ø¹Ù„ Ù‰ â–Ø®Ø¯Ù… Ø© â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø§Ù„Ù„ÙŠ â–Ø®Ø¯Ø§Ù…ÙŠÙ† â–ÙˆÙ„ Ø© â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø§Ù„Ù„ÙŠ â–Ø®Ø¯Ø§Ù…ÙŠÙ† â–ÙÙŠ â–Ù„Ù‚Ø·Ø§Ø¹ â–Ù„Ø®Ø§Øµ â–Ø¹ÙŠÙˆÙ† â–Ù„ÙƒÙ„Ø§Ù… â–ØªØµÙ†ÙŠÙ â–Ø¬Ù‡ Ø© â–Ø¯Ø±Ø¹ Ø© â–ØªØ§ÙÙŠÙ„Ø§Ù„Øª â–xxbos â–Ø¥ÙŠ Ù… ÙŠÙ„ Ù…Ø§ ÙŠØ³ â–Ø¬Ù…Ø§Ø¹ Ø© â–ØªØ±Ø§Ø¨ÙŠØ© â–Ù‚Ø±Ùˆ ÙŠØ© â–ÙƒØ§ÙŠÙ† | Ø´ â–ÙŠÙ‚Ø±Ø§ Ùˆ â–ÙˆÙ„Ø§ â–ÙŠÙƒØªØ¨ Ùˆ â–Ù†Ø³Ø¨Ø© â–ÙƒØ§Ù† â– Ù‚Ø§Ø±ÙŠÙŠÙ† â–ÙÙˆÙ‚ â– Ø§Ù†ÙˆÙŠ â–ØªØ§Ù†ÙˆÙŠ â–Ø¬Ø§Ù…Ø¹ Ø© â–Ø§Ù‚ØªØµØ§Ø¯ â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø´ÙŠØ· ÙŠÙ† â– ÙŠÙ‚Ø¯Ø±Ùˆ â– ÙŠØ®Ø¯Ù…Ùˆ â–Ù†Ø³Ø¨Ø© â–Ù„Ø¨Ø·Ø§Ù„ Ø© â–Ø§Ø³ â–Ù…Ø§ â–Ø®Ø¯Ø§Ù…ÙŠÙ† Ø´ â–ØªØ§ÙŠÙ‚ Ù„Ø¨Ùˆ â–Ø¹Ù„ Ù‰ â–Ø®Ø¯Ù… Ø© â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø§Ù„Ù„ÙŠ â–Ø®Ø¯Ø§Ù…ÙŠÙ† â–ÙˆÙ„ Ø© â–Ù†Ø³Ø¨Ø© â–Ø§Ø³ â–Ø§Ù„Ù„ÙŠ â–Ø®Ø¯Ø§Ù…ÙŠÙ† â–ÙÙŠ â–Ù„Ù‚Ø·Ø§Ø¹ â–Ù„Ø®Ø§Øµ â–Ø¹ÙŠÙˆÙ† â–Ù„ÙƒÙ„Ø§Ù… â–ØªØµÙ†ÙŠÙ â–Ø¬Ù‡ Ø© â–Ø¯Ø±Ø¹ Ø© â–ØªØ§ÙÙŠÙ„Ø§Ù„Øª â–xxbos â–Ø¥ÙŠ Ù… ÙŠÙ„ Ù…Ø§ ÙŠØ³ â–Ø¬Ù…Ø§Ø¹ Ø© â–ØªØ±Ø§Ø¨ÙŠØ© â–Ù‚Ø±Ùˆ ÙŠØ© â–ÙƒØ§ÙŠÙ† Ø© | . Special tokens in fastai start with letters xx, they are useful to help our model handle the shift from original text to our preprocessed text. For example, xxunk is used to replace the tokens that don&#39;t exist in our vocab, as it can be useful to help our model learn to deal with missing tokens. . print(dls_lm.vocab[:20]) . [&#39;xxunk&#39;, &#39;xxpad&#39;, &#39;xxbos&#39;, &#39;xxeos&#39;, &#39;xxfld&#39;, &#39;xxrep&#39;, &#39;xxwrep&#39;, &#39;xxup&#39;, &#39;xxmaj&#39;, &#39;â–&#39;, &#39;Ø©&#39;, &#39;Ùˆ&#39;, &#39;Ø§&#39;, &#39;â–Ù„&#39;, &#39;â–Ø§Ù„&#39;, &#39;Ø´&#39;, &#39;Ù…&#39;, &#39;Ø§Øª&#39;, &#39;Ù„&#39;, &#39;Ø±&#39;] . Now it&#39;s time to create our language model learner, we pass it our dataloaders, and use the version of AWD_LSTM provided by fastai. . Perplexity is usually used to evaluate language models, a model with a low perplexity is one that assigns a high probabilty to the correct output, in our case, the model learns by trying to predict the next token in a sequence, so the lower the perplexity the better is our model at predicting the next token correctly. Perplexity is a good metric to look at when training your language model and tuning the different hyperparamters, but i think the best way to measure the quality of a language model is to actually apply it on a task (text classifcation, question ansewring, ...) and look at your accuracy (or any other metric) going up or down. . learn = language_model_learner(dls_lm, AWD_LSTM, metrics=[accuracy, Perplexity()], pretrained=False) . AWD_LSTM is just LSTM layers with lots of regularization, and we can see that in the hyperparamters below in fastai&#39;s implementation, all the parameters that end with &#39;p&#39; are the ammount of dropout applied to some part of the network. Other regularization techniques are also used, like activation regularization which is similar to weight decay but applied to the activations instead of the weights. Another interesting technique is weight tying, that is based on the intuition that out embedding layer is a mapping from darija to a vector representation, and our output layer is a mapping from a vector representation back to darija, so why not use the same weight matrix for both of them, this turn out to be a useful method to reduce the number of parameters of a language model especially when we have a huge vocabulary size. . awd_lstm_lm_config . {&#39;bidir&#39;: False, &#39;emb_sz&#39;: 400, &#39;embed_p&#39;: 0.02, &#39;hidden_p&#39;: 0.15, &#39;input_p&#39;: 0.25, &#39;n_hid&#39;: 1152, &#39;n_layers&#39;: 3, &#39;out_bias&#39;: True, &#39;output_p&#39;: 0.1, &#39;pad_token&#39;: 1, &#39;tie_weights&#39;: True, &#39;weight_p&#39;: 0.2} . Time to train our model using the one cycle policy, that was introduced in this paper. It is a method that suggests varying the learning rate linearly in two steps, one where we go from a small learning rate to a maximum learning rate that we specify (in this case max_lr=1e-2), then decreasing it to a lower value than the one we started with. . . Starting from a low learning rate is used as a warm-up step and has the effect of allowing the model to get used to the data before jumping to a high learning rate, when we reach the maximum learning rate, it acts as a regularization that helps the model escape saddle points and avoid steep areas of the loss and prefer a flatter minimum, that we can navigate while decreasing our learning rate in the second step. . learn.fit_one_cycle(n_epoch=50, lr_max=1e-2) . epoch train_loss valid_loss accuracy perplexity time . 0 | 5.515688 | 5.475324 | 0.047372 | 238.727844 | 01:51 | . 1 | 5.483952 | 5.467685 | 0.048320 | 236.911163 | 01:52 | . 2 | 4.817659 | 4.317418 | 0.184282 | 74.994751 | 01:52 | . 3 | 4.145897 | 4.027465 | 0.210001 | 56.118462 | 01:51 | . 4 | 3.805228 | 3.598531 | 0.272597 | 36.544495 | 01:52 | . 5 | 3.457134 | 3.275732 | 0.328941 | 26.462589 | 01:52 | . 6 | 3.179850 | 3.041847 | 0.368637 | 20.943880 | 01:51 | . 7 | 2.990458 | 2.920819 | 0.386745 | 18.556480 | 01:51 | . 8 | 2.897987 | 2.895197 | 0.395819 | 18.087059 | 01:51 | . 9 | 2.838742 | 2.817706 | 0.407951 | 16.738407 | 01:51 | . 10 | 2.816499 | 2.769104 | 0.413117 | 15.944349 | 01:51 | . 11 | 2.764984 | 2.747474 | 0.416647 | 15.603168 | 01:51 | . 12 | 2.767782 | 2.733072 | 0.417585 | 15.380055 | 01:51 | . 13 | 2.739008 | 2.728491 | 0.418405 | 15.309772 | 01:51 | . 14 | 2.736031 | 2.713049 | 0.423188 | 15.075172 | 01:51 | . 15 | 3.146243 | 3.005011 | 0.370182 | 20.186436 | 01:51 | . 16 | 3.078941 | 2.861412 | 0.396981 | 17.486204 | 01:51 | . 17 | 2.923705 | 2.777859 | 0.411634 | 16.084555 | 01:51 | . 18 | 2.810098 | 2.730683 | 0.419738 | 15.343361 | 01:51 | . 19 | 2.774838 | 2.704668 | 0.424419 | 14.949357 | 01:51 | . 20 | 2.715703 | 2.685028 | 0.426555 | 14.658613 | 01:51 | . 21 | 2.686922 | 2.668929 | 0.430696 | 14.424510 | 01:51 | . 22 | 2.646046 | 2.639531 | 0.435448 | 14.006639 | 01:51 | . 23 | 2.618499 | 2.626800 | 0.438193 | 13.829449 | 01:51 | . 24 | 2.614452 | 2.616818 | 0.440093 | 13.692085 | 01:51 | . 25 | 2.544441 | 2.593445 | 0.443312 | 13.375767 | 01:51 | . 26 | 2.539290 | 2.582659 | 0.445333 | 13.232270 | 01:51 | . 27 | 2.507925 | 2.571883 | 0.447520 | 13.090456 | 01:51 | . 28 | 2.475592 | 2.561796 | 0.449330 | 12.959073 | 01:51 | . 29 | 2.453772 | 2.546964 | 0.451991 | 12.768286 | 01:54 | . 30 | 2.400351 | 2.543346 | 0.454446 | 12.722167 | 01:51 | . 31 | 2.382363 | 2.533504 | 0.456875 | 12.597570 | 01:51 | . 32 | 2.350547 | 2.526517 | 0.458009 | 12.509854 | 01:51 | . 33 | 2.323781 | 2.517891 | 0.459875 | 12.402414 | 01:51 | . 34 | 2.312293 | 2.512608 | 0.462032 | 12.337064 | 01:51 | . 35 | 2.294926 | 2.503262 | 0.464382 | 12.222299 | 01:51 | . 36 | 2.278101 | 2.503091 | 0.465769 | 12.220203 | 01:51 | . 37 | 2.247103 | 2.497797 | 0.467440 | 12.155691 | 01:51 | . 38 | 2.232022 | 2.491923 | 0.468650 | 12.084488 | 01:51 | . 39 | 2.204907 | 2.491375 | 0.468941 | 12.077871 | 01:51 | . 40 | 2.196845 | 2.489012 | 0.470028 | 12.049372 | 01:51 | . 41 | 2.179559 | 2.486800 | 0.470606 | 12.022747 | 01:51 | . 42 | 2.187601 | 2.483679 | 0.470737 | 11.985275 | 01:51 | . 43 | 2.140077 | 2.483773 | 0.470888 | 11.986403 | 01:52 | . 44 | 2.160355 | 2.482263 | 0.471528 | 11.968322 | 01:51 | . 45 | 2.146067 | 2.483346 | 0.471368 | 11.981293 | 01:51 | . 46 | 2.124532 | 2.482664 | 0.471880 | 11.973123 | 01:52 | . 47 | 2.144808 | 2.483089 | 0.471457 | 11.978213 | 01:51 | . 48 | 2.124281 | 2.482518 | 0.471581 | 11.971367 | 01:51 | . 49 | 2.128707 | 2.482447 | 0.471589 | 11.970522 | 01:51 | . learn.save(model_path/&#39;darija_lm&#39;) . Path(&#39;/content/drive/MyDrive/ml/projects/darija/models/darija_lm.pth&#39;) . def decoder(sentence): s = &#39;&#39;.join(sentence) return s.split(&#39;â–&#39;) . We use the predict method to generate two sentences with 100 subwords each, it will take a piece of text as input and start doing the usual work of predicting the next token. We don&#39;t just take the word with the highest probability but we randomly sample from a probability distribution (the output of the softmax), this is done because we want our model to be a little creative and not just keep repeating itself; a high temperature will smooth this probability distribution and give tokens with low probability a higher chance of being sampled. . text = &#39;Ø±Ø³Ø§Ù… ØµØ§Ù†Ø¹ Ø·Ø¨Ø§Ø¹Ø©&#39; n_toks = 100 n_sentences = 2 preds = [learn.predict(text, n_toks, temperature=0.75, decoder=decoder) for _ in range(n_sentences)] . This is the output of our model, we have ourselves a drunk GPT-3 ğŸ˜„, but we can see that it&#39;s able to generate words correctly even though we are using subwords, this is more apparent when we look at the output of our model without the decoder below. . preds . [&#39; xxbos Ø±Ø³Ø§Ù… ØµØ§Ù†Ø¹ Ø·Ø¨Ø§Ø¹Ø© Ù„Ù„ÙÙ†ÙˆÙ† Ø¨Ù„ÙÙˆØ±Ù…Ø© Ù„Ø¨Ø±ØªØºØ§Ù„ÙŠØ© electura cafÃ©cie Ù…ÙƒØªÙˆØ¨ Ù…Ù† Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ù‡ÙŠ Ù…Ù† ÙƒØªØ§Ø¨ ØªØµÙÙŠØ§Øª ÙØ§Ù„Ø·Ø±ÙŠÙ‚ Ø§Ù„Ù„ÙŠ Ø±Ø¨Ø­ Ù„Ø¨ÙˆØ·ÙˆÙ„Ø§Øª Ø¯ÙŠØ§Ù„ ØµØ§Ù…ÙˆØ§ Ø§Ù„Ø¯Ø§Ù†Ù…Ø§Ø±Ùƒ Ù…Ø³ØªØ¹Ù…Ø±Ø© ÙÙŠØ§Ø³ÙˆØ§Øª Ø·ÙˆÙŠÙ„Ø© ÙˆÙ‚Ø¹Ø§Øª ÙŠÙ†Ø§ÙŠØ± ÙƒØ§ØªØ¨Ø© Ùˆ&#39;, &#39; xxbos Ø±Ø³Ø§Ù… ØµØ§Ù†Ø¹ Ø·Ø¨Ø§Ø¹Ø© Ù…Ø¯Ø±ÙŠØ¯ ØªÙ…Ø§Ø´Ù‚ Ø­Ø¯ÙŠØ¯ÙŠØ© Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¯ÙŠØ§Ù„ Ø£ØªÙ„ØªÙŠÙƒÙˆ Ù…Ø¯Ø±ÙŠØ¯ ÙˆØ²Ø§Ø±Ø© Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ© ØªØ²Ø§Ø¯Øª Ø£ÙƒØªÙˆØ¨Ø± ÙˆØ³Ù„Ø§Ù…Ø© ÙˆØ³Ù‡ÙˆÙ„ Ù¾Ø±ÙŠÙ…Ùˆ Ø³Ùƒ Ø§Ù† Ø£Ù„Ù…Ø§Ù†ÙŠØ§ Ù„ÙŠÙ…Ù† Ø¯ÙŠØ§Ù„ Ø£Ù„Ù…Ø§Ù†ÙŠØ§ Ù„ÙŠ ÙƒØ§ØªÙƒÙˆÙ† Ø¯ÙŠÙƒØ§Ø±Ø· Ø¨Ø­Ø±ÙŠØ© Ù…Ø¹ Ø§Ù„Ø¨Ø·ÙˆÙ„Ø© Ø¯ÙŠØ§Ù„Ùˆ Ø¹Ù„Ù‰ Ø¨ÙŠØªÙƒÙˆÙŠÙ† Ù„Ø¨Ø§Ø´Ø§Ø± Ù…ÙˆØ±Ø§ Ù…Ø§ ØªØ¹Ø·Ø§Ùˆ&#39;] . text = &#39;Ø±Ø³Ø§Ù… ØµØ§Ù†Ø¹ Ø·Ø¨Ø§Ø¹Ø©&#39; n_toks = 100 n_sentences = 2 preds = [learn.predict(text, n_toks, temperature=0.75) for _ in range(n_sentences)] . The output without a decoder, tokens are separated by spaces, while the actual space is replaced by â–, you might notice that the output is different because we are randomly sampling. . preds . [&#39;â–xxbos â– Ø± Ø³ Ø§Ù… â– Øµ Ø§Ù† Ø¹ â– Ø· Ø¨ Ø§Ø¹ Ø© â–Øª Ù Ø§Ø­ Ø© â– Ø´ Ù ÙŠÙ‚ â–Ù… Ø® Øª Ø§Ù„ Ù Ø© â–Ø¹Ù„ Ù‰ â– Ø´ Ø­ Ø§Ù„ â–Ù…Ù† â–ÙˆØ§Ø­Ø¯ â–Ø§Ù„Ø³ Ø® Ø§Ù† Ø© â–ÙƒØ¨ÙŠØ± Ø© â–ÙƒØª Ø± â–Ù…Ù† â– Ù‡Ø§ Ùƒ â– Ø§Ùƒ â–Ù… Øº Ø§Ø± Ø¨ÙŠ Ø© â–ÙƒØ§Øª Ø¹ Ø· Ù‰ â–Ù„Ù„ Ø­ Ø¨ Ø³ â– ÙŠØ§ â–Ø£ Ùƒ Ø« Ø± â–Ù…Ù† Ù‡Ø§ â– Ø´ Ùƒ Ù„ â–Ù… Ù… Ù„ Ùƒ Ø© â–Ù…Ùˆ Ø±ÙŠ Ø·Ø§Ù† ÙŠØ© â–Ù‡Ø§Ø¯ Ø´ÙŠ â–Ù„ÙŠ â–Ø® Ù„Ø§ â– Ù†Ø§Ø³ â– Ø± Ø¯ Ø§Øª Ùˆ â–Ù„Ù… Øª Øº ÙŠØ± Ø§Øª â–Ø¯ ÙŠÙ† ÙŠØ© â–Ø¹Ù„ Ù‰ â–Ù„ Ùˆ Øº Ø§Øª â– Ù†Øª Ø¬ Ø§Øª&#39;, &#39;â–xxbos â– Ø± Ø³ Ø§Ù… â– Øµ Ø§Ù† Ø¹ â– Ø· Ø¨ Ø§Ø¹ Ø© â–Ù… Øµ Ø¯Ø± â–Ù…Ø­ Ù„ ÙˆÙ„ â–Øª Ø® ØªØ§Ø± Ø¹ Ø§Øª â–Ø¹Ù„ Ù‰ â–Ø§Ù„Ù… Ø¬ Ø§Ù„ â–Ø§Ù„ Ø¢ Ø®Ø± â–Ø¹Ù„ Ù‰ â–Ø­ Ù‚ Ùˆ Ù‚ â–Ø¨Ù† Ø§Ø¯ Ù… â– Ø´ ÙˆÙ â–Ø­ØªØ§ â– Ø³Øª Ø§ Ù‚Ù„ Ø§Øª â–Ø¹ Ø§ ÙˆÙ† Ø§Øª Ù‡Ø§ â–Ù„Ù‚ Ø± Ø¢ Ù† â–Ù…Ù† â–Ø¨Ø¹Ø¯ â–Ù…Ø§ Ø®Ø¯ Ø§ Ù‡Ø§ â–Ù„Ø¹ Ø¯ Ø¯ â–Ø¯ÙŠØ§Ù„ â–Ø§Ù„ Ù†Ø§Ø³ â–Ù„ÙŠ â–ÙƒØ§Ù† â–Ø¹Ù†Ø¯ Ù‡ÙˆÙ… â–Ù… Ø¬Ù„ Ø© â–Ù„Ù… Ø±Ùƒ Ø²ÙŠ Ø© â– Ø¯Ø± ÙŠØ© â–Ø¹ Ø§Ø¯ â– Ø·Ù„ Ø¹ Ø§Øª â–Ø¨Ø§Ø´ â–Øª Ø¹ Ø·Ø§ â–Ù„Ù„ Ù‚ Ø¨Ø§ ÙŠÙ„ ÙŠØ© â–Ù„Ù… Ø¤ Ø³ Ø³ Ø§Øª â–Ø¯ÙŠØ§Ù„ â–Ø§Ù„ Ù†Ø§Ø³ â–ÙˆÙ„ ÙƒÙ† â–ÙƒØ§Øª ÙƒÙˆÙ† â–ÙÙŠÙ‡Ø§&#39;] .",
            "url": "https://issam9.github.io/ml-blog/2021/08/30/Darija-LM.html",
            "relUrl": "/2021/08/30/Darija-LM.html",
            "date": " â€¢ Aug 30, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Iâ€™m Abderrahmane Issam, i recently completed a masterâ€™s in Cloud and High Performance Computing, and iâ€™m interested and passionate about Machine Learning, so thatâ€™s what i write about in this blog. .",
          "url": "https://issam9.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://issam9.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}